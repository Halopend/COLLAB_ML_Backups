# COLLAB_ML_Backups
Backups of ML assignments I did for one of my courses (the computational side, not really the cleaned up presented reports).

Most projects made heavy use of tensor board to which I had created extensions to allow for "visualization generators" to be swapped in/out as needed since I believed the key to gaining insights and improving the functionality was through well organized viualizations.
In addition, I would run small sample size test automated to brute force the best performing methods while attempting to consider the balance between learning rate and final accuracy (though generally speaking it was more useful as a way of quickly weeding out techniques not as useful or performant to a given dataset given the very finite time I had with training I always needed to consider). Basically, I found ways to cleanly test/compare 10-20 techniques (including permuations on parameters within those techniques) to quickly narrow in on what worked best (using small but sufficient sample sizes randomly chosen).

Note most of these are not functional with the latest setups of collab, but their techniques/codebas can still be useful as a reference point (at least for myself).
