{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"A4-CNN.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"5UoogDkgZfpz","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"d43e683a-d316-4495-f8ac-b63bf92f3291"},"source":["import tensorflow.compat.v1 as tf\n","tf.disable_v2_behavior() \n","\n","import numpy as np\n","\n","batch_size = 128\n","test_size = 256\n","channel_depth = 3\n","num_catecories = 10\n","\n","## A parameter representing how much max pool is reducing the complexity\n","## Example: 2 implies the image is halved in height/width\n","max_pool_reduction_ratio = 2 ####IMPORTANT: this ratio should evenly divide the image size\n","\n","def init_weights(shape):\n","    return tf.Variable(tf.random_normal(shape, stddev=0.01))\n","\n","def model(X, w, w_fc, w_o, p_keep_conv, p_keep_hidden):\n","    l1a = tf.nn.relu(tf.nn.conv2d(X, w,                       # l1a shape=(?, 28, 28, 32)\n","                        strides=[1, 1, 1, 1], padding='SAME'))\n","    l1 = tf.nn.max_pool(l1a, ksize=[1, max_pool_reduction_ratio, max_pool_reduction_ratio, 1],              # l1 shape=(?, 14, 14, 32)\n","                        strides=[1, 2, 2, 1], padding='SAME')\n","    l1 = tf.nn.dropout(l1, p_keep_conv)\n","\n","\n","    l3 = tf.reshape(l1, [-1, w_fc.get_shape().as_list()[0]])    # reshape to (?, 14x14x32)\n","\n","    l4 = tf.nn.relu(tf.matmul(l3, w_fc))\n","    l4 = tf.nn.dropout(l4, p_keep_hidden)\n","\n","    pyx = tf.matmul(l4, w_o)\n","    return pyx\n","\n","\n"," # Load data\n","def load_data(nb_classes=10):\n","    from keras.datasets import cifar10\n","    from keras.utils import np_utils\n","    from keras import backend\n","    backend.set_image_data_format('channels_last')   \n","    # the data, shuffled and split between train and test sets\n","    (X_train, y_train), (X_test, y_test) = cifar10.load_data()\n","\n","    def prepare_X(X):\n","        return X.astype('float32') / 255\n","\n","    X_train, X_test = [prepare_X(X) for X in (X_train, X_test)]\n","    print(X_train.shape[0], 'train image samples of shape: ', X_train.shape[1:])\n","    print(X_test.shape[0], 'test samples')\n","\n","    # convert class vectors to binary class matrices (basically one hot)\n","    Y_train = np_utils.to_categorical(y_train, nb_classes)\n","    Y_test = np_utils.to_categorical(y_test, nb_classes)\n","\n","    w = X_train[0].shape[0]\n","    h = X_train[0].shape[1]\n","\n","    # Fixes datasets whose shape don't imply depth (example, MNIST shape is 28x28 instead of 28,28,1)\n","    if (np.size(X_train[0].shape) == 2):\n","      X_train = X_train.reshape(-1, w, h, channel_depth)  # 28x28x1 input img (in the case of MNIST)\n","      X_test = X_test.reshape(-1, w, h,   channel_depth)  # 28x28x1 input img (in the case of MNIST)    \n","    \n","    return X_train, Y_train, X_test, Y_test, (w,h)\n","\n","trX, trY, teX, teY, shape = load_data()\n","\n","# Lets load our placeholders automatically based on the shape of the dataset\n","X = tf.placeholder(\"float\", [None, shape[0], shape[1], channel_depth])\n","Y = tf.placeholder(\"float\", [None, num_catecories])\n","\n","### This is the most important section. Here we decide the actual dimensions of our neural network\n","##  for simplicity, we will do just a 3x3 convolve, but we need to make sure the depth is included.\n","filter_size = [8,8]\n","filter_depth = channel_depth \n","conv_depth= 64               # this is the number of layers we want to feed into the next step and is essentially how many (possibly colour) filters we want to find\n","\n","num_outputs = 800            # number of outputs before the category outputs to give us tunable weight to increase the accuracy of the output layer\n","pool_dimensions = [conv_depth* int(shape[0]/max_pool_reduction_ratio) * int(shape[1]/max_pool_reduction_ratio),num_outputs]\n","\n","\n","w = init_weights([filter_size[0], filter_size[1], filter_depth, conv_depth])       # example 3x3x3x32 => 32, 3 channel filters. The window size is the 3x3\n","w_fc = init_weights(pool_dimensions)                                               # example: a 28x28 image with a 2 reduction ratio, becomes a 14x14 image in t\n","w_o = init_weights([num_outputs, num_catecories])                                  # FC 625 inputs, 10 outputs (labels)\n","p_keep_conv = tf.placeholder(\"float\")\n","p_keep_hidden = tf.placeholder(\"float\")\n","py_x = model(X, w, w_fc, w_o, p_keep_conv, p_keep_hidden)\n","\n","\n","cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=py_x, labels=Y))\n","train_op = tf.train.RMSPropOptimizer(0.001, 0.9).minimize(cost)\n","predict_op = tf.argmax(py_x, 1)\n","\n","# Launch the graph in a session\n","with tf.Session() as sess:\n","    # you need to initialize all variables\n","    tf.global_variables_initializer().run()\n","\n","    for i in range(100):\n","        training_batch = zip(range(0, len(trX), batch_size),\n","                             range(batch_size, len(trX)+1, batch_size))\n","        for start, end in training_batch:\n","            sess.run(train_op, feed_dict={X: trX[start:end], Y: trY[start:end], p_keep_conv: 0.8, p_keep_hidden: 0.5})\n","\n","        test_indices = np.arange(len(teX)) # Get A Test Batch\n","        np.random.shuffle(test_indices)\n","        test_indices = test_indices[0:test_size]\n","\n","        print(i, np.mean(np.argmax(teY[test_indices], axis=1) ==\n","                         sess.run(predict_op, feed_dict={X: teX[test_indices],\n","                                                         p_keep_conv: 1.0,\n","                                                         p_keep_hidden: 1.0})))\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","non-resource variables are not supported in the long term\n","Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n","170500096/170498071 [==============================] - 11s 0us/step\n","50000 train image samples of shape:  (32, 32, 3)\n","10000 test samples\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:201: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:201: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","\n","Future major versions of TensorFlow will allow gradients to flow\n","into the labels input on backprop by default.\n","\n","See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/rmsprop.py:123: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Call initializer instance with the dtype argument instead of passing it to the constructor\n","0 0.34375\n","1 0.44921875\n","2 0.6015625\n","3 0.62109375\n","4 0.64453125\n","5 0.61328125\n","6 0.64453125\n","7 0.63671875\n","8 0.66015625\n","9 0.69921875\n","10 0.703125\n","11 0.671875\n","12 0.65234375\n","13 0.671875\n","14 0.69921875\n","15 0.64453125\n","16 0.67578125\n","17 0.66796875\n","18 0.6640625\n","19 0.6875\n","20 0.68359375\n","21 0.64453125\n","22 0.7265625\n","23 0.703125\n","24 0.65625\n","25 0.640625\n","26 0.734375\n","27 0.65625\n","28 0.71875\n","29 0.6640625\n","30 0.6640625\n","31 0.69921875\n","32 0.703125\n","33 0.67578125\n","34 0.68359375\n","35 0.7109375\n","36 0.69921875\n","37 0.7265625\n","38 0.71875\n","39 0.7265625\n","40 0.68359375\n","41 0.71484375\n","42 0.69140625\n","43 0.6640625\n","44 0.6328125\n","45 0.671875\n","46 0.6015625\n","47 0.6796875\n","48 0.6328125\n","49 0.6796875\n","50 0.68359375\n","51 0.69140625\n","52 0.69921875\n","53 0.65625\n","54 0.703125\n","55 0.60546875\n","56 0.72265625\n","57 0.69140625\n","58 0.6640625\n","59 0.59765625\n","60 0.66015625\n","61 0.70703125\n","62 0.64453125\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"1mFNPKTfcFJV","colab_type":"code","colab":{}},"source":["sess.run(predict_op, feed_dict={X: teX[test_indices],\n","                                                         p_keep_conv: 1.0,\n","                                                         p_keep_hidden: 1.0})"],"execution_count":null,"outputs":[]}]}